{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "da_transfomers-intro.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "18B0xgqOVXDq"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQ5JN+25N12WDbLrxF9JiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PeerChristensen/NLP-Demos/blob/main/da_transfomers_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUHwuaqIaSTA"
      },
      "source": [
        "# An overview of Danish transfomer models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18B0xgqOVXDq"
      },
      "source": [
        "## Named entity recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhFw8WQJah3N"
      },
      "source": [
        "We get the current best model for Danish NER. It can be found [here](\"https://huggingface.co/saattrupdan/nbailab-base-ner-scandi\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH9ZkjLgRplI",
        "outputId": "d9e39456-3aad-47a1-8cea-4ffaf8fd08f5"
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "model = 'saattrupdan/nbailab-base-ner-scandi'\n",
        "ner = pipeline(\"ner\", model=model, aggregation_strategy='first', )"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF1NNdImUGmv"
      },
      "source": [
        "text = \"Margrethe Laursen, bosiddende på adressen Vibevej 25 i København, blev indlagt på Bispebjerg Hospital efter en ulykke i forbindelse med hendes arbejde ved Movia. Hun blev behandlet af Overlæge Jens Severinsen.\""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UrXQ_pPT-Fm",
        "outputId": "df535ed2-7374-45b5-aba5-fc0032522cc2"
      },
      "source": [
        "ner(text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'end': 17,\n",
              "  'entity_group': 'PER',\n",
              "  'score': 0.99971926,\n",
              "  'start': 0,\n",
              "  'word': 'Margrethe Laursen'},\n",
              " {'end': 52,\n",
              "  'entity_group': 'LOC',\n",
              "  'score': 0.9973518,\n",
              "  'start': 42,\n",
              "  'word': 'Vibevej 25'},\n",
              " {'end': 64,\n",
              "  'entity_group': 'LOC',\n",
              "  'score': 0.99921095,\n",
              "  'start': 55,\n",
              "  'word': 'København'},\n",
              " {'end': 101,\n",
              "  'entity_group': 'LOC',\n",
              "  'score': 0.9718465,\n",
              "  'start': 82,\n",
              "  'word': 'Bispebjerg Hospital'},\n",
              " {'end': 160,\n",
              "  'entity_group': 'ORG',\n",
              "  'score': 0.9937564,\n",
              "  'start': 155,\n",
              "  'word': 'Movia'},\n",
              " {'end': 208,\n",
              "  'entity_group': 'PER',\n",
              "  'score': 0.94952404,\n",
              "  'start': 193,\n",
              "  'word': 'Jens Severinsen'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW3Y5XkpbBRx"
      },
      "source": [
        "Given the standard output, we can make a function that anonymizes text by removing named entities based on character positions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtIn6ZBFS_l4"
      },
      "source": [
        "def find_and_remove_named_entities(text: str) -> str:\n",
        "    \"\"\"Use current best NER model (saattrupdan/nbailab-base-ner-scandi) to identify named entities.\n",
        "    Entities are removed by position ranges within strings.\n",
        "    The model and pipeline are defined outside this function.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        named_ents = ner(text)\n",
        "        ranges_to_remove = [range(i[\"start\"], i[\"end\"]) for i in named_ents]\n",
        "        new_text = ''.join([char for idx, char in enumerate(text) if not any(idx in rng for rng in ranges_to_remove)])\n",
        "        return new_text\n",
        "    except:\n",
        "        return text"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xh062PG2TALG",
        "outputId": "8227ff48-ec27-40b9-b19c-8527800edb3b"
      },
      "source": [
        "find_and_remove_named_entities(text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "', bosiddende på adressen  i , blev indlagt på  efter en ulykke i forbindelse med hendes arbejde ved . Hun blev behandlet af Overlæge .'"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vyrA7ojeavk"
      },
      "source": [
        "## Translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUQF3INJmHKK"
      },
      "source": [
        "### A quick example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX-te4mimGYi",
        "outputId": "ef8bfc6a-4f7a-4c04-dedc-903bd4f84392"
      },
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-da\",truncation=True, max_length=500)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-da\")\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Jeg ønsker at leve, jeg vil give. Jeg har været en minearbejder for et hjerte af guld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gtji6KCmoyO",
        "outputId": "bd3fc9f7-fb1d-4925-8ed3-e14f64f495cd"
      },
      "source": [
        "translation = pipeline(\"translation_en_to_da\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "text = \"I want to live, I want to give. I've been a miner for a heart of gold\"\n",
        "\n",
        "translated_text = translation(text)[0]['translation_text']\n",
        "print(translated_text)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jeg ønsker at leve, jeg vil give. Jeg har været en minearbejder for et hjerte af guld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc7OmqamnrXD"
      },
      "source": [
        "### A not so quick *example*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0dTZ03woMPx"
      },
      "source": [
        "In this example, we'll see how to translate The Da Vinci Code in .epub format into Danish"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYJ2AdgFehDc",
        "outputId": "8aff6d09-d94a-42ba-e85e-5d88928b2674"
      },
      "source": [
        "!pip install epub-conversion\n",
        "!pip install xml_cleaner\n",
        "\n",
        "from epub_conversion.utils import open_book, convert_epub_to_lines\n",
        "import re, time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: epub-conversion in /usr/local/lib/python3.7/dist-packages (1.0.15)\n",
            "Requirement already satisfied: epub in /usr/local/lib/python3.7/dist-packages (from epub-conversion) (0.5.2)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.7/dist-packages (from epub-conversion) (0.98)\n",
            "Requirement already satisfied: ciseau in /usr/local/lib/python3.7/dist-packages (from epub-conversion) (1.0.1)\n",
            "Requirement already satisfied: xml_cleaner in /usr/local/lib/python3.7/dist-packages (2.0.4)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j31PJpJfhAdD"
      },
      "source": [
        "#### Preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbeZtcxiNJ5"
      },
      "source": [
        "def clean_text(text):\n",
        "  cleanr = re.compile('<.*?>')\n",
        "  cleantext = re.sub(cleanr, '', text)\n",
        "  return cleantext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZcfxLlShLIJ"
      },
      "source": [
        "book = open_book(\"/Users/peerchristensen/Downloads/DaVinciCode.epub\")\n",
        "\n",
        "lines = convert_epub_to_lines(book)\n",
        "\n",
        "cleaned_text = [clean_text(line) for line in lines]\n",
        "\n",
        "cleaned_text = [text.strip() for text in cleaned_text]\n",
        "\n",
        "cleaned_text = list(filter(None, cleaned_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-X8x9yXp1ZV"
      },
      "source": [
        "We can use a dataframe to store the original and translated text to better evaluate the quality of the translations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOpBNDR-iznN"
      },
      "source": [
        "df = pd.DataFrame({'text': cleaned_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xZTp8riqo6l"
      },
      "source": [
        "#### Translate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po50rh_Aq82S"
      },
      "source": [
        "def translate(text):\n",
        "    if text is None or text == \"\":\n",
        "        return \"Error\",\n",
        "\n",
        "    #batch input + sentence tokenization\n",
        "    batch = tokenizer.prepare_seq2seq_batch(sent_tokenize(text))\n",
        "\n",
        "    #run model\n",
        "    translated = model.generate(**batch)\n",
        "    tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "    return \" \".join(tgt_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYpZ_RIxrWpx"
      },
      "source": [
        "df['translated'] = df[\"clean_text\"].map(lambda x: translate(x)).copy()\n",
        "\n",
        "df.to_csv('translated_auto.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}